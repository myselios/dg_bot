# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a Bitcoin AI automated trading bot that uses OpenAI GPT-4 for trading decisions. The system runs on a 1-hour interval schedule using APScheduler, executing trades on the Upbit exchange based on AI analysis of technical indicators and market data.

**Tech Stack**: Python 3.11, FastAPI, PostgreSQL, Docker, APScheduler, OpenAI GPT-4, TA-Lib

## Common Commands

### Environment Setup
```bash
# Activate virtual environment (venv)
# Windows PowerShell
.\venv\Scripts\Activate.ps1

# Windows CMD
venv\Scripts\activate.bat

# Linux/Mac
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
# requirements-api.txtê°€ requirements.txtì— í†µí•©ë¨
```

### Testing
```bash
# Run all tests (must be in venv)
python -m pytest tests/ -v

# Run specific test file
python -m pytest tests/test_module_name.py -v

# Run tests with coverage
python -m pytest tests/ --cov=src --cov=backend --cov-report=html

# Run only unit tests
python -m pytest tests/ -m unit -v

# Run only integration tests
python -m pytest tests/ -m integration -v

# Run scheduler tests
python -m pytest tests/backend/app/core/test_scheduler.py -v

# Run a single test
python -m pytest tests/test_module.py::TestClass::test_method -v
```

### Running the Bot

```bash
# Run trading cycle once (development)
python main.py

# Run scheduler (1-hour interval automated trading)
python scheduler_main.py

# Run with Docker (scheduler only)
docker-compose up -d scheduler
docker-compose logs -f scheduler

# Run full stack (DB, API, monitoring)
docker-compose up -d
```

### Docker Operations
```bash
# Build and run scheduler
docker-compose build scheduler
docker-compose up -d scheduler

# View logs
docker-compose logs -f scheduler

# Stop services
docker-compose down

# Clean rebuild
docker-compose down -v
docker-compose build --no-cache
docker-compose up -d
```

### Database Operations
```bash
# The database is managed by Docker Compose
# PostgreSQL runs in a container with persistent volume
# Tables are auto-created via SQLAlchemy on first run

# Access PostgreSQL container
docker exec -it dg_bot-postgres-1 psql -U postgres -d trading_bot
```

## Architecture

### System Flow

The bot operates on a **dual-mode architecture**:

1. **Standalone Mode** (`main.py`): Single execution of trading cycle
2. **Scheduler Mode** (`scheduler_main.py`): Automated 1-hour interval execution

**Key Flow**:
```
scheduler_main.py
  â†’ backend/app/core/scheduler.py (APScheduler configuration)
    â†’ trading_job() calls main.execute_trading_cycle()
      â†’ QuickBacktestFilter (rule-based filtering, no AI)
      â†’ SignalAnalyzer (technical analysis)
      â†’ AIService (GPT-4 decision making)
      â†’ TradingService (order execution)
      â†’ Database recording (via backend models)
      â†’ Telegram notifications
```

### Directory Structure

```
dg_bot/
â”œâ”€â”€ main.py                    # Main trading cycle (standalone execution)
â”œâ”€â”€ scheduler_main.py          # Scheduler entry point (automated mode)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ container.py           # DI Container (Clean Architecture)
â”‚   â”‚
â”‚   â”œâ”€â”€ domain/                # ğŸ†• Domain Layer (Pure Business Logic)
â”‚   â”‚   â”œâ”€â”€ entities/          # Trade, Order, Position entities
â”‚   â”‚   â”œâ”€â”€ value_objects/     # Money, Percentage value objects
â”‚   â”‚   â”œâ”€â”€ services/          # FeeCalculator, RiskCalculator
â”‚   â”‚   â””â”€â”€ exceptions.py
â”‚   â”‚
â”‚   â”œâ”€â”€ application/           # ğŸ†• Application Layer (Use Cases)
â”‚   â”‚   â”œâ”€â”€ ports/outbound/    # Port interfaces (ExchangePort, AIPort, etc.)
â”‚   â”‚   â”œâ”€â”€ use_cases/         # ExecuteTradeUseCase, AnalyzeMarketUseCase
â”‚   â”‚   â””â”€â”€ dto/               # Data Transfer Objects
â”‚   â”‚
â”‚   â”œâ”€â”€ infrastructure/        # ğŸ†• Infrastructure Layer (Adapters)
â”‚   â”‚   â””â”€â”€ adapters/
â”‚   â”‚       â”œâ”€â”€ exchange/      # UpbitExchangeAdapter
â”‚   â”‚       â”œâ”€â”€ ai/            # OpenAIAdapter
â”‚   â”‚       â”œâ”€â”€ market_data/   # UpbitMarketDataAdapter
â”‚   â”‚       â”œâ”€â”€ persistence/   # InMemoryPersistenceAdapter
â”‚   â”‚       â””â”€â”€ legacy_bridge.py  # Legacy service wrappers
â”‚   â”‚
â”‚   â”œâ”€â”€ presentation/          # ğŸ†• Presentation Layer
â”‚   â”‚   â””â”€â”€ cli/               # TradingRunner CLI
â”‚   â”‚
â”‚   â”œâ”€â”€ ai/                    # AI decision making (GPT-4) - Legacy
â”‚   â”‚   â”œâ”€â”€ service.py         # AIService - main AI analysis
â”‚   â”‚   â””â”€â”€ market_correlation.py
â”‚   â”œâ”€â”€ api/                   # Exchange API clients - Legacy
â”‚   â”‚   â””â”€â”€ upbit_client.py    # Upbit exchange integration
â”‚   â”œâ”€â”€ backtesting/           # Backtesting engine
â”‚   â”‚   â”œâ”€â”€ backtester.py      # Main backtesting engine
â”‚   â”‚   â”œâ”€â”€ quick_filter.py    # Fast rule-based filtering
â”‚   â”‚   â”œâ”€â”€ rule_based_strategy.py  # Rule-based strategy
â”‚   â”‚   â””â”€â”€ ai_strategy.py     # AI-based strategy
â”‚   â”œâ”€â”€ data/                  # Data collection
â”‚   â”‚   â””â”€â”€ collector.py       # Market data collector
â”‚   â”œâ”€â”€ trading/               # Trading execution
â”‚   â”‚   â”œâ”€â”€ service.py         # TradingService - order execution
â”‚   â”‚   â”œâ”€â”€ indicators.py      # Technical indicators (RSI, MACD, etc.)
â”‚   â”‚   â””â”€â”€ signal_analyzer.py # Signal analysis
â”‚   â”œâ”€â”€ position/              # Position management
â”‚   â””â”€â”€ config/                # Configuration
â”‚       â””â”€â”€ settings.py        # All configuration classes
â”œâ”€â”€ backend/                   # FastAPI backend + database
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py           # FastAPI application entry
â”‚   â”‚   â”œâ”€â”€ api/v1/           # REST API endpoints
â”‚   â”‚   â”‚   â””â”€â”€ endpoints/
â”‚   â”‚   â”‚       â”œâ”€â”€ bot_control.py  # Bot control API
â”‚   â”‚   â”‚       â””â”€â”€ trades.py       # Trade history API
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py     # Backend settings
â”‚   â”‚   â”‚   â””â”€â”€ scheduler.py  # APScheduler configuration
â”‚   â”‚   â”œâ”€â”€ db/               # Database setup
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py       # SQLAlchemy base
â”‚   â”‚   â”‚   â”œâ”€â”€ session.py    # DB session management
â”‚   â”‚   â”‚   â””â”€â”€ init_db.py    # DB initialization
â”‚   â”‚   â”œâ”€â”€ models/           # SQLAlchemy ORM models
â”‚   â”‚   â”‚   â”œâ”€â”€ trade.py      # Trade records
â”‚   â”‚   â”‚   â”œâ”€â”€ ai_decision.py # AI decision logs
â”‚   â”‚   â”‚   â”œâ”€â”€ order.py      # Order records
â”‚   â”‚   â”‚   â”œâ”€â”€ portfolio.py  # Portfolio snapshots
â”‚   â”‚   â”‚   â”œâ”€â”€ bot_config.py # Bot configuration
â”‚   â”‚   â”‚   â””â”€â”€ system_log.py # System logs
â”‚   â”‚   â”œâ”€â”€ schemas/          # Pydantic schemas
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚       â”œâ”€â”€ trading_engine.py  # Trading logic integration
â”‚   â”‚       â”œâ”€â”€ notification.py    # Telegram notifications
â”‚   â”‚       â””â”€â”€ metrics.py         # Prometheus metrics
â”‚   â””â”€â”€ tests/                # Backend tests
â”œâ”€â”€ tests/                    # Main tests directory
â”œâ”€â”€ scripts/backtesting/      # Data collection scripts
â”œâ”€â”€ monitoring/               # Prometheus + Grafana configs
â””â”€â”€ docs/                     # Documentation (ALL .md files go here)
```

### Key Components

**AIService** (`src/ai/service.py`):
- Uses OpenAI GPT-4 for trading decisions
- Analyzes technical indicators, market trends, and chart patterns
- Returns decision with confidence score and reasoning
- Methods: `analyze()`, `prepare_analysis_data()`

**TradingService** (`src/trading/service.py`):
- Executes buy/sell orders via Upbit API
- Handles fee calculation, slippage, split orders
- Methods: `buy()`, `sell()`, `calculate_fee()`, `calculate_slippage()`

**QuickBacktestFilter** (`src/backtesting/quick_filter.py`):
- Fast rule-based filtering WITHOUT AI calls
- Uses rule-based strategy to pre-filter trading opportunities
- Significantly reduces AI API costs

**Scheduler** (`backend/app/core/scheduler.py`):
- APScheduler configuration for 1-hour interval jobs
- `trading_job()`: Main scheduled task
- Handles error recovery, duplicate prevention (max_instances=1)

**Database Models** (`backend/app/models/`):
- `Trade`: Executed trades
- `AIDecision`: AI analysis logs
- `Order`: Order details
- `Portfolio`: Portfolio snapshots
- All use SQLAlchemy ORM with async sessions

### Data Flow

1. **Data Collection**: `DataCollector` fetches OHLCV data from Upbit
2. **Technical Analysis**: `TechnicalIndicators` calculates RSI, MACD, Bollinger Bands
3. **Quick Filter**: `QuickBacktestFilter` applies rule-based strategy (no AI cost)
4. **Signal Analysis**: `SignalAnalyzer` analyzes buy/sell signals
5. **AI Decision**: `AIService` makes final decision via GPT-4
6. **Order Execution**: `TradingService` executes trade on Upbit
7. **Database Recording**: Models store trade, decision, and portfolio data
8. **Notifications**: Telegram alerts sent via `notification.py`
9. **Metrics**: Prometheus metrics recorded via `metrics.py`

### Clean Architecture (Hexagonal/Ports & Adapters)

The project implements Clean Architecture for better testability and maintainability:

```
Presentation â†’ Application â†’ Domain
      â†“              â†“
Infrastructure â”€â”€â”€â”€â”€â”˜
```

**Key Concepts**:
- **Domain Layer**: Pure business logic (Trade, Order, Position, Money, Percentage)
- **Application Layer**: Use cases and port interfaces (ExchangePort, AIPort, etc.)
- **Infrastructure Layer**: Adapters for external systems (Upbit, OpenAI, PostgreSQL)
- **Presentation Layer**: CLI runner and schedulers

**DI Container Usage**:
```python
from src.container import Container

# Production
container = Container()
execute_trade = container.get_execute_trade_use_case()

# Testing with mocks
container = Container.create_for_testing()

# Legacy service migration
container = Container.create_from_legacy(
    upbit_client=existing_upbit,
    ai_service=existing_ai
)
```

**Testing by Layer**:
```bash
# Domain layer only (no mocks needed)
python -m pytest tests/unit/domain/ -v

# Use cases (with port mocks)
python -m pytest tests/unit/application/ -v

# Adapters (integration tests)
python -m pytest tests/unit/infrastructure/ -v
```

## Development Guidelines

### TDD (Test-Driven Development)

This project strictly follows TDD principles (see `.cursorrules`):

1. **Red**: Write failing test first
2. **Green**: Write minimal code to pass test
3. **Refactor**: Optimize code while keeping tests green

**Test Structure**:
- All tests in `tests/` directory
- Use `@pytest.mark.unit` for unit tests
- Use `@pytest.mark.integration` for integration tests
- Fixtures in `conftest.py`
- Follow Given-When-Then pattern

### Virtual Environment (venv)

**CRITICAL**: Always use venv for Python commands
- Location: `venv/` (project root)
- Activate before running any Python commands
- Never run Python commands outside venv

### File Organization Rules

From `.cursorrules`:

1. **Documentation**: ALL `.md` files MUST go in `docs/` (except root `README.md`, `CLAUDE.md`)
2. **Scripts**: Development scripts in project root, data scripts in `scripts/`
3. **Temporary Files**: Delete temporary test scripts after use

### Documentation Structure (IMPORTANT)

ë¬¸ì„œëŠ” ë°˜ë“œì‹œ ì•„ë˜ êµ¬ì¡°ë¥¼ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤:

```
docs/
â”œâ”€â”€ guide/                     # ê°€ì´ë“œ ë¬¸ì„œ (ì‚¬ìš©ë²•, ì„¤ì •ë²•)
â”‚   â”œâ”€â”€ ARCHITECTURE.md        # ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜
â”‚   â”œâ”€â”€ DOCKER_GUIDE.md        # Docker ì‹¤í–‰ ê°€ì´ë“œ
â”‚   â”œâ”€â”€ MONITORING_GUIDE.md    # Grafana/Prometheus ëª¨ë‹ˆí„°ë§
â”‚   â”œâ”€â”€ RISK_MANAGEMENT_CONFIG.md  # ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì„¤ì •
â”‚   â”œâ”€â”€ SCHEDULER_GUIDE.md     # ìŠ¤ì¼€ì¤„ëŸ¬ ê°€ì´ë“œ
â”‚   â”œâ”€â”€ TELEGRAM_SETUP_GUIDE.md    # Telegram ì•Œë¦¼ ì„¤ì •
â”‚   â””â”€â”€ USER_GUIDE.md          # ì‚¬ìš©ì ê°€ì´ë“œ
â”œâ”€â”€ plans/                     # ê³„íš/ì²´í¬ë¦¬ìŠ¤íŠ¸ ë¬¸ì„œ
â”‚   â””â”€â”€ PLAN_*.md              # êµ¬í˜„ ê³„íš, ë¦¬íŒ©í† ë§ ê³„íš ë“±
â””â”€â”€ diagrams/                  # ë‹¤ì´ì–´ê·¸ë¨ íŒŒì¼
```

**ë¬¸ì„œ ê´€ë¦¬ ê·œì¹™**:
- ìƒˆ ê°€ì´ë“œ ë¬¸ì„œ â†’ `docs/guide/`ì— ìƒì„±
- êµ¬í˜„ ê³„íš, ì²´í¬ë¦¬ìŠ¤íŠ¸ â†’ `docs/plans/`ì— ìƒì„±
- ì¼íšŒì„± ë³´ê³ ì„œ (ë¦¬íŒ©í† ë§ ë³´ê³ ì„œ, ë³€ê²½ ë¡œê·¸ ë“±) â†’ ìƒì„±í•˜ì§€ ì•ŠìŒ
- ëª¨ë“  ë¬¸ì„œëŠ” `**ì‘ì„±ì¼**: YYYY-MM-DD` í˜•ì‹ì˜ ë‚ ì§œ í¬í•¨ í•„ìˆ˜
- ë¬¸ì„œ ìˆ˜ì • ì‹œ ë‚ ì§œ ì—…ë°ì´íŠ¸ í•„ìˆ˜

### Windows Encoding (PowerShell)

If running on Windows, PowerShell scripts need UTF-8 encoding setup:

```powershell
# Add to start of .ps1 files
[Console]::OutputEncoding = [System.Text.Encoding]::UTF8
$OutputEncoding = [System.Text.Encoding]::UTF8
chcp 65001 | Out-Null
```

Prefer `.bat` files over `.ps1` for Windows scripts when possible.

## Configuration

### Environment Variables

All configuration is in `.env` file (copy from `env.example`):

**Required**:
- `UPBIT_ACCESS_KEY`: Upbit API access key
- `UPBIT_SECRET_KEY`: Upbit API secret key
- `OPENAI_API_KEY`: OpenAI API key

**Optional but Recommended**:
- `TELEGRAM_BOT_TOKEN`: Telegram bot token
- `TELEGRAM_CHAT_ID`: Telegram chat ID
- `SENTRY_DSN`: Sentry error tracking DSN
- `POSTGRES_*`: Database configuration (for Docker)
- `PROMETHEUS_ENABLED`: Enable Prometheus metrics
- `GRAFANA_*`: Grafana configuration

### Configuration Classes

All in `src/config/settings.py`:
- `TradingConfig`: Trading parameters (fee rate, min trade, etc.)
- `AIConfig`: AI model settings (GPT-4, temperature, etc.)
- `DataConfig`: Data collection settings (intervals, counts)
- `BacktestConfig`: Backtesting parameters
- Backend config in `backend/app/core/config.py`

## Important Notes

### Trading Safety

1. **Start Small**: Test with minimal amounts first
2. **Monitor Closely**: Check logs and Telegram notifications
3. **API Key Security**: NEVER commit `.env` file
4. **Dry Run**: Use backtesting (`python backtest.py`) before live trading

### Scheduler Behavior

- Runs every 1 hour (configurable in `scheduler.py`)
- `max_instances=1` prevents concurrent executions
- Graceful shutdown on SIGINT/SIGTERM
- Auto-recovery on errors (logs to Sentry if enabled)

### Database

- PostgreSQL required for production (via Docker)
- Tables auto-created on first run via SQLAlchemy
- Async sessions throughout backend
- Migrations not implemented (using declarative_base auto-create)

### AI Costs

- Each AI decision call costs money (GPT-4 API)
- `QuickBacktestFilter` reduces AI calls by pre-filtering with rules
- Monitor usage via OpenAI dashboard

### Backtesting

Two modes:
1. **Rule-based only**: No AI calls, fast and free
2. **AI-based**: Includes GPT-4 analysis, slower and costly

Use `scripts/backtesting/` for historical data collection.

## Common Issues

### Import Errors
- Ensure venv is activated
- Check `PYTHONPATH` includes project root
- `scheduler_main.py` adds project root to `sys.path`

### Database Connection
- Verify PostgreSQL is running (`docker-compose ps`)
- Check `DATABASE_URL` in `.env`
- Ensure async driver: `postgresql+asyncpg://...`

### API Errors
- Verify API keys in `.env`
- Check Upbit/OpenAI rate limits
- Review logs in `logs/` directory

### Scheduler Not Running
- Check `scheduler.log` in `logs/scheduler/`
- Verify no other instance is running
- Check system time (scheduler uses Asia/Seoul timezone)

## Useful References

- [User Guide](docs/guide/USER_GUIDE.md): Complete user documentation
- [Scheduler Guide](docs/guide/SCHEDULER_GUIDE.md): Scheduler detailed guide
- [Docker Guide](docs/guide/DOCKER_GUIDE.md): Docker setup and deployment
- [Architecture](docs/guide/ARCHITECTURE.md): System architecture details
- [Risk Management](docs/guide/RISK_MANAGEMENT_CONFIG.md): Risk management configuration
- [Monitoring Guide](docs/guide/MONITORING_GUIDE.md): Grafana/Prometheus setup
- [Telegram Setup](docs/guide/TELEGRAM_SETUP_GUIDE.md): Telegram notifications
