"""
ê³¼ê±° ë°ì´í„° ë™ê¸°í™” ê´€ë¦¬ì (Historical Data Sync)

ë°±í…ŒìŠ¤íŒ…ì„ ìœ„í•œ ê³¼ê±° ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ê´€ë¦¬í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ì‹ ê·œ ì½”ì¸: ì „ì²´ ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ìµœëŒ€ 2ë…„)
- ê¸°ì¡´ ì½”ì¸: ì¦ë¶„ ì—…ë°ì´íŠ¸
- ë°ì´í„° ìœ íš¨ì„± ê²€ì¦
- ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬ (3ë…„ ì´ìƒ)
- íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ (API ë¬´ì‘ë‹µ ë°©ì§€)
"""
import asyncio
import os
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
from pathlib import Path
import pandas as pd
import pyupbit

from src.utils.logger import Logger


@dataclass
class SyncStatus:
    """ë°ì´í„° ë™ê¸°í™” ìƒíƒœ"""
    ticker: str
    symbol: str
    status: str  # 'success', 'partial', 'failed', 'skipped'
    rows_before: int
    rows_after: int
    rows_added: int
    date_range: Optional[Tuple[datetime, datetime]] = None
    error_message: Optional[str] = None
    sync_time: datetime = field(default_factory=datetime.now)


class HistoricalDataSync:
    """
    ê³¼ê±° ë°ì´í„° ë™ê¸°í™” ê´€ë¦¬ì

    ë°±í…ŒìŠ¤íŒ…ì„ ìœ„í•œ ê³¼ê±° ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ê´€ë¦¬í•©ë‹ˆë‹¤.

    ì‚¬ìš© ì˜ˆì‹œ:
        sync = HistoricalDataSync(data_dir="./data/historical")
        status = await sync.sync_coin_data("KRW-BTC", years=2)
        print(f"ë™ê¸°í™” ê²°ê³¼: {status.status}, ì¶”ê°€ëœ í–‰: {status.rows_added}")
    """

    # Upbit API ì œí•œ
    MAX_CANDLES_PER_REQUEST = 200  # í•œ ë²ˆì— ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” ìµœëŒ€ ìº”ë“¤ ìˆ˜
    API_DELAY_SECONDS = 0.15  # API í˜¸ì¶œ ê°„ê²©
    API_TIMEOUT_SECONDS = 30  # API í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
    SYNC_TIMEOUT_SECONDS = 60  # ë‹¨ì¼ ì½”ì¸ ë™ê¸°í™” íƒ€ì„ì•„ì›ƒ (ì´ˆ)

    def __init__(
        self,
        data_dir: str = "./data/historical",
        default_years: int = 2,
        max_years: int = 3
    ):
        """
        Args:
            data_dir: ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
            default_years: ê¸°ë³¸ ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„ (ë…„)
            max_years: ìµœëŒ€ ë³´ê´€ ê¸°ê°„ (ë…„), ì´ˆê³¼ ì‹œ ì‚­ì œ
        """
        self.data_dir = Path(data_dir)
        self.default_years = default_years
        self.max_years = max_years
        self._executor = ThreadPoolExecutor(max_workers=5, thread_name_prefix="data_sync")

        # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
        self.data_dir.mkdir(parents=True, exist_ok=True)

    def get_data_path(self, ticker: str, interval: str = "day") -> Path:
        """ë°ì´í„° íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
        symbol = ticker.replace("KRW-", "")
        return self.data_dir / f"{symbol}_{interval}.parquet"

    async def sync_coin_data(
        self,
        ticker: str,
        years: Optional[int] = None,
        interval: str = "day",
        force_full: bool = False
    ) -> SyncStatus:
        """
        íŠ¹ì • ì½”ì¸ ë°ì´í„° ë™ê¸°í™”

        Args:
            ticker: ì½”ì¸ í‹°ì»¤ (ì˜ˆ: KRW-BTC)
            years: ìˆ˜ì§‘ ê¸°ê°„ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            interval: ë°ì´í„° ê°„ê²© ('day', 'minute60', 'minute240')
            force_full: Trueë©´ ì „ì²´ ì¬ë‹¤ìš´ë¡œë“œ

        Returns:
            SyncStatus: ë™ê¸°í™” ê²°ê³¼
        """
        symbol = ticker.replace("KRW-", "")
        years = years or self.default_years
        data_path = self.get_data_path(ticker, interval)

        Logger.print_info(f"ğŸ“¥ [{symbol}] ë°ì´í„° ë™ê¸°í™” ì‹œì‘...")

        try:
            # ê¸°ì¡´ ë°ì´í„° í™•ì¸
            existing_df = None
            rows_before = 0

            if data_path.exists() and not force_full:
                existing_df = pd.read_parquet(data_path)
                rows_before = len(existing_df)
                Logger.print_info(f"  ê¸°ì¡´ ë°ì´í„°: {rows_before}í–‰")

            # ì‹œì‘ ë‚ ì§œ ê²°ì •
            if existing_df is not None and len(existing_df) > 0:
                # ì¦ë¶„ ì—…ë°ì´íŠ¸: ë§ˆì§€ë§‰ ë°ì´í„° ë‹¤ìŒ ë‚ ë¶€í„°
                last_date = pd.Timestamp(existing_df.index[-1])
                start_date = last_date + timedelta(days=1)
                Logger.print_info(f"  ì¦ë¶„ ì—…ë°ì´íŠ¸: {start_date.date()} ~")
            else:
                # ì „ì²´ ë‹¤ìš´ë¡œë“œ: yearsë…„ ì „ë¶€í„°
                start_date = datetime.now() - timedelta(days=years * 365)
                Logger.print_info(f"  ì „ì²´ ë‹¤ìš´ë¡œë“œ: {start_date.date()} ~")

            # í˜„ì¬ ë‚ ì§œ
            end_date = datetime.now()

            # ë°ì´í„° ìˆ˜ì§‘ í•„ìš” ì—¬ë¶€ í™•ì¸
            if start_date >= end_date:
                return SyncStatus(
                    ticker=ticker,
                    symbol=symbol,
                    status='skipped',
                    rows_before=rows_before,
                    rows_after=rows_before,
                    rows_added=0,
                    date_range=(existing_df.index[0], existing_df.index[-1]) if existing_df is not None else None
                )

            # ë°ì´í„° ìˆ˜ì§‘
            new_df = await self._fetch_historical_data(
                ticker=ticker,
                start_date=start_date,
                end_date=end_date,
                interval=interval
            )

            if new_df is None or len(new_df) == 0:
                Logger.print_warning(f"  ìƒˆë¡œìš´ ë°ì´í„° ì—†ìŒ")
                return SyncStatus(
                    ticker=ticker,
                    symbol=symbol,
                    status='skipped' if rows_before > 0 else 'failed',
                    rows_before=rows_before,
                    rows_after=rows_before,
                    rows_added=0
                )

            # ë°ì´í„° ë³‘í•©
            if existing_df is not None and len(existing_df) > 0:
                # ì¤‘ë³µ ì œê±°í•˜ë©° ë³‘í•©
                combined_df = pd.concat([existing_df, new_df])
                combined_df = combined_df[~combined_df.index.duplicated(keep='last')]
                combined_df = combined_df.sort_index()
            else:
                combined_df = new_df

            # ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬
            cutoff_date = datetime.now() - timedelta(days=self.max_years * 365)
            combined_df = combined_df[combined_df.index >= cutoff_date]

            # ì €ì¥
            combined_df.to_parquet(data_path)
            rows_after = len(combined_df)

            Logger.print_success(f"  ì™„ë£Œ: {rows_after}í–‰ (ì¶”ê°€: {rows_after - rows_before}í–‰)")

            return SyncStatus(
                ticker=ticker,
                symbol=symbol,
                status='success',
                rows_before=rows_before,
                rows_after=rows_after,
                rows_added=rows_after - rows_before,
                date_range=(combined_df.index[0], combined_df.index[-1])
            )

        except Exception as e:
            Logger.print_error(f"  ë™ê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return SyncStatus(
                ticker=ticker,
                symbol=symbol,
                status='failed',
                rows_before=rows_before,
                rows_after=rows_before,
                rows_added=0,
                error_message=str(e)
            )

    async def sync_multiple_coins(
        self,
        tickers: List[str],
        years: Optional[int] = None,
        interval: str = "day",
        max_concurrent: int = 3
    ) -> List[SyncStatus]:
        """
        ì—¬ëŸ¬ ì½”ì¸ ë°ì´í„° ë™ê¸°í™”

        Args:
            tickers: ì½”ì¸ í‹°ì»¤ ëª©ë¡
            years: ìˆ˜ì§‘ ê¸°ê°„
            interval: ë°ì´í„° ê°„ê²©
            max_concurrent: ë™ì‹œ ì²˜ë¦¬ ìˆ˜ (API ì œí•œ ê³ ë ¤)

        Returns:
            SyncStatus ë¦¬ìŠ¤íŠ¸
        """
        Logger.print_header(f"ğŸ“¦ ë©€í‹° ì½”ì¸ ë°ì´í„° ë™ê¸°í™” ({len(tickers)}ê°œ)")

        results = []
        semaphore = asyncio.Semaphore(max_concurrent)

        async def sync_with_semaphore(ticker: str) -> SyncStatus:
            """íƒ€ì„ì•„ì›ƒì´ ì ìš©ëœ ë™ê¸°í™”"""
            async with semaphore:
                try:
                    # ê°œë³„ ì½”ì¸ ë™ê¸°í™”ì— íƒ€ì„ì•„ì›ƒ ì ìš©
                    return await asyncio.wait_for(
                        self.sync_coin_data(ticker, years, interval),
                        timeout=self.SYNC_TIMEOUT_SECONDS
                    )
                except asyncio.TimeoutError:
                    symbol = ticker.replace("KRW-", "")
                    Logger.print_error(f"  [{symbol}] â° ë™ê¸°í™” íƒ€ì„ì•„ì›ƒ ({self.SYNC_TIMEOUT_SECONDS}ì´ˆ)")
                    return SyncStatus(
                        ticker=ticker,
                        symbol=symbol,
                        status='failed',
                        rows_before=0,
                        rows_after=0,
                        rows_added=0,
                        error_message=f"ë™ê¸°í™” íƒ€ì„ì•„ì›ƒ ({self.SYNC_TIMEOUT_SECONDS}ì´ˆ)"
                    )
                except Exception as e:
                    symbol = ticker.replace("KRW-", "")
                    Logger.print_error(f"  [{symbol}] âŒ ë™ê¸°í™” ì‹¤íŒ¨: {str(e)}")
                    return SyncStatus(
                        ticker=ticker,
                        symbol=symbol,
                        status='failed',
                        rows_before=0,
                        rows_after=0,
                        rows_added=0,
                        error_message=str(e)
                    )

        # ë³‘ë ¬ ì²˜ë¦¬ (ì „ì²´ì—ë„ íƒ€ì„ì•„ì›ƒ ì ìš©)
        tasks = [sync_with_semaphore(ticker) for ticker in tickers]
        try:
            # ì „ì²´ ë™ê¸°í™” ì‘ì—…ì— 3ë¶„ íƒ€ì„ì•„ì›ƒ
            results = await asyncio.wait_for(
                asyncio.gather(*tasks, return_exceptions=True),
                timeout=180  # 3ë¶„
            )
        except asyncio.TimeoutError:
            Logger.print_error(f"âŒ ì „ì²´ ë™ê¸°í™” íƒ€ì„ì•„ì›ƒ (3ë¶„)")
            # ì™„ë£Œë˜ì§€ ì•Šì€ ì‘ì—…ì€ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬
            results = []
            for ticker in tickers:
                results.append(SyncStatus(
                    ticker=ticker,
                    symbol=ticker.replace("KRW-", ""),
                    status='failed',
                    rows_before=0,
                    rows_after=0,
                    rows_added=0,
                    error_message="ì „ì²´ ë™ê¸°í™” íƒ€ì„ì•„ì›ƒ"
                ))

        # ì˜ˆì™¸ ì²˜ë¦¬
        final_results = []
        for ticker, result in zip(tickers, results):
            if isinstance(result, Exception):
                Logger.print_error(f"  [{ticker}] ì˜ˆì™¸ ë°œìƒ: {str(result)}")
                final_results.append(SyncStatus(
                    ticker=ticker,
                    symbol=ticker.replace("KRW-", ""),
                    status='failed',
                    rows_before=0,
                    rows_after=0,
                    rows_added=0,
                    error_message=str(result)
                ))
            else:
                final_results.append(result)

        # ê²°ê³¼ ìš”ì•½
        success_count = sum(1 for r in final_results if r.status == 'success')
        failed_count = sum(1 for r in final_results if r.status == 'failed')
        Logger.print_info(f"\nğŸ“Š ë™ê¸°í™” ì™„ë£Œ: ì„±ê³µ {success_count}/{len(tickers)}, ì‹¤íŒ¨ {failed_count}")

        return final_results

    async def _fetch_historical_data(
        self,
        ticker: str,
        start_date: datetime,
        end_date: datetime,
        interval: str
    ) -> Optional[pd.DataFrame]:
        """ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ (í˜ì´ì§• ì²˜ë¦¬, íƒ€ì„ì•„ì›ƒ ì ìš©)"""
        all_data = []
        current_to = end_date
        max_retries = 3

        def fetch_ohlcv(t: str, intv: str, cnt: int, to_str: str):
            """ë™ê¸° API í˜¸ì¶œ (í´ë¡œì € ë¬¸ì œ ë°©ì§€ë¥¼ ìœ„í•´ ëª…ì‹œì  ì¸ì ì „ë‹¬)"""
            return pyupbit.get_ohlcv(t, interval=intv, count=cnt, to=to_str)

        while current_to > start_date:
            retry_count = 0
            df = None

            while retry_count < max_retries:
                try:
                    # íƒ€ì„ì•„ì›ƒì´ ìˆëŠ” API í˜¸ì¶œ
                    to_str = current_to.strftime("%Y-%m-%d %H:%M:%S")
                    df = await asyncio.wait_for(
                        asyncio.get_event_loop().run_in_executor(
                            self._executor,
                            fetch_ohlcv,
                            ticker,
                            interval,
                            self.MAX_CANDLES_PER_REQUEST,
                            to_str
                        ),
                        timeout=self.API_TIMEOUT_SECONDS
                    )
                    break  # ì„±ê³µ ì‹œ ë£¨í”„ íƒˆì¶œ

                except asyncio.TimeoutError:
                    retry_count += 1
                    if retry_count >= max_retries:
                        Logger.print_warning(f"  [{ticker}] API íƒ€ì„ì•„ì›ƒ (ì¬ì‹œë„ {max_retries}íšŒ ì‹¤íŒ¨)")
                        return None if not all_data else pd.concat(all_data).sort_index()
                    await asyncio.sleep(1)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°

                except Exception as e:
                    Logger.print_warning(f"  ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {str(e)}")
                    return None if not all_data else pd.concat(all_data).sort_index()

            if df is None or len(df) == 0:
                break

            # ì‹œì‘ ë‚ ì§œ ì´í›„ ë°ì´í„°ë§Œ í•„í„°ë§
            df = df[df.index >= start_date]
            all_data.append(df)

            # ë‹¤ìŒ í˜ì´ì§€ ê³„ì‚°
            earliest = df.index[0]
            if earliest <= start_date:
                break

            current_to = earliest - timedelta(seconds=1)

            # API ì œí•œ ë°©ì§€
            await asyncio.sleep(self.API_DELAY_SECONDS)

        if not all_data:
            return None

        # ë°ì´í„° ë³‘í•© ë° ì •ë ¬
        combined = pd.concat(all_data)
        combined = combined[~combined.index.duplicated(keep='first')]
        combined = combined.sort_index()

        return combined

    def load_data(self, ticker: str, interval: str = "day") -> Optional[pd.DataFrame]:
        """
        ì €ì¥ëœ ë°ì´í„° ë¡œë“œ

        Args:
            ticker: ì½”ì¸ í‹°ì»¤
            interval: ë°ì´í„° ê°„ê²©

        Returns:
            DataFrame ë˜ëŠ” None
        """
        data_path = self.get_data_path(ticker, interval)

        if not data_path.exists():
            return None

        try:
            return pd.read_parquet(data_path)
        except Exception as e:
            Logger.print_error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ({ticker}): {str(e)}")
            return None

    def get_data_info(self, ticker: str, interval: str = "day") -> Optional[Dict]:
        """ë°ì´í„° ì •ë³´ ì¡°íšŒ"""
        data_path = self.get_data_path(ticker, interval)

        if not data_path.exists():
            return None

        try:
            df = pd.read_parquet(data_path)
            return {
                'ticker': ticker,
                'interval': interval,
                'rows': len(df),
                'start_date': df.index[0],
                'end_date': df.index[-1],
                'file_size_mb': data_path.stat().st_size / (1024 * 1024),
                'columns': list(df.columns)
            }
        except Exception as e:
            return {'error': str(e)}

    def cleanup_old_data(self) -> Dict[str, int]:
        """ì˜¤ë˜ëœ ë°ì´í„° íŒŒì¼ ì •ë¦¬"""
        cutoff_date = datetime.now() - timedelta(days=self.max_years * 365)
        cleaned = {'files_deleted': 0, 'rows_removed': 0}

        for file_path in self.data_dir.glob("*.parquet"):
            try:
                df = pd.read_parquet(file_path)
                original_len = len(df)

                # ì˜¤ë˜ëœ ë°ì´í„° ì œê±°
                df = df[df.index >= cutoff_date]

                if len(df) < original_len:
                    if len(df) > 0:
                        df.to_parquet(file_path)
                        cleaned['rows_removed'] += original_len - len(df)
                    else:
                        # ëª¨ë“  ë°ì´í„°ê°€ ì‚­ì œë¨ - íŒŒì¼ ì‚­ì œ
                        file_path.unlink()
                        cleaned['files_deleted'] += 1
                        cleaned['rows_removed'] += original_len

            except Exception as e:
                Logger.print_warning(f"ì •ë¦¬ ì‹¤íŒ¨ ({file_path.name}): {str(e)}")

        return cleaned

    def print_data_summary(self) -> None:
        """ì €ì¥ëœ ë°ì´í„° ìš”ì•½ ì¶œë ¥"""
        Logger.print_header("ğŸ“Š ì €ì¥ëœ ë°ì´í„° ìš”ì•½")

        files = list(self.data_dir.glob("*.parquet"))
        if not files:
            print("  ì €ì¥ëœ ë°ì´í„° ì—†ìŒ")
            return

        total_size = 0
        print(f"{'íŒŒì¼':>20} {'í–‰ìˆ˜':>10} {'ê¸°ê°„':>25} {'í¬ê¸°(MB)':>10}")
        print("-" * 70)

        for file_path in sorted(files):
            try:
                df = pd.read_parquet(file_path)
                size_mb = file_path.stat().st_size / (1024 * 1024)
                total_size += size_mb

                period = f"{df.index[0].strftime('%Y-%m-%d')} ~ {df.index[-1].strftime('%Y-%m-%d')}"
                print(f"{file_path.name:>20} {len(df):>10,} {period:>25} {size_mb:>10.2f}")

            except Exception as e:
                print(f"{file_path.name:>20} ì˜¤ë¥˜: {str(e)}")

        print("-" * 70)
        print(f"{'ì´ê³„':>20} {len(files)}ê°œ íŒŒì¼, {total_size:.2f} MB")
